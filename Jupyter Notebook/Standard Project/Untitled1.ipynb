{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GramMatrix(nn.Module):\n",
    "    \n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()\n",
    "        features = input.view(a * b, c * d)\n",
    "        G = torch.mm(features, features.t())\n",
    "        \n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class StyleCNN(object):\n",
    "    def __init__(self, style, content, pastiche):\n",
    "        super(StyleCNN, self).__init__()\n",
    "        \n",
    "        self.style = style\n",
    "        self.content = content\n",
    "        self.pastiche = nn.Parameter(pastiche.data)\n",
    "        \n",
    "        self.content_layers = ['conv_4']\n",
    "        self.style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "        self.content_weight = 1\n",
    "        self.style_weight = 1000\n",
    "        \n",
    "        self.loss_network = models.vgg19(pretrained=True)\n",
    "        \n",
    "        self.gram = GramMatrix()\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.optimizer = optim.LBFGS([self.pastiche])\n",
    "        \n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "        if self.use_cuda:\n",
    "            self.loss_network.cuda()\n",
    "            self.gram.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self):\n",
    "    def closure():\n",
    "        self.optimizer.zero_grad()\n",
    "          \n",
    "        pastiche = self.pastiche.clone()\n",
    "        pastiche.data.clamp_(0, 1)\n",
    "        content = self.content.clone()\n",
    "        style = self.style.clone()\n",
    "            \n",
    "        content_loss = 0\n",
    "        style_loss = 0\n",
    "            \n",
    "        i = 1\n",
    "        not_inplace = lambda layer: nn.ReLU(inplace=False) if isinstance(layer, nn.ReLU) else layer\n",
    "        for layer in list(self.loss_network.features):\n",
    "            layer = not_inplace(layer)\n",
    "            if self.use_cuda:\n",
    "                layer.cuda()\n",
    "                \n",
    "            pastiche, content, style = layer.forward(pastiche), layer.forward(content), layer.forward(style)\n",
    "            \n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                name = \"conv_\" + str(i)\n",
    "                    \n",
    "                if name in self.content_layers:\n",
    "                    content_loss += self.loss(pastiche * self.content_weight, content.detach() * self.content_weight)\n",
    "                    \n",
    "                if name in self.style_layers:\n",
    "                    pastiche_g, style_g = self.gram.forward(pastiche), self.gram.forward(style)\n",
    "                    style_loss += self.loss(pastiche_g * self.style_weight, style_g.detach() * self.style_weight)\n",
    "                \n",
    "            if isinstance(layer, nn.ReLU):\n",
    "                i += 1\n",
    "            \n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "            \n",
    "        return total_loss\n",
    "\n",
    "    self.optimizer.step(closure)\n",
    "    return self.pastiche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AN\\lib\\site-packages\\torchvision\\transforms\\transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "import scipy.misc\n",
    "\n",
    "imsize = 256\n",
    "\n",
    "loader = transforms.Compose([\n",
    "             transforms.Scale(imsize),\n",
    "             transforms.ToTensor()\n",
    "         ])\n",
    "\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    image = Variable(loader(image))\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "  \n",
    "def save_image(input, path):\n",
    "    image = input.data.clone().cpu()\n",
    "    image = image.view(3, imsize, imsize)\n",
    "    image = unloader(image)\n",
    "    scipy.misc.imsave(path, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
